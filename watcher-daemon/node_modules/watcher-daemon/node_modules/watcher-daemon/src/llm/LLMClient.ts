import axios, { AxiosInstance } from 'axios';
import Logger from '../logger/Logger';
import { OllamaGenerateResponse, TokenUsage } from './types';

export class LLMClient {
  private client: AxiosInstance;
  private model: string;
  private tokenUsage: TokenUsage;

  constructor(host: string, model: string) {
    this.client = axios.create({
      baseURL: host,
      timeout: 30000,
    });
    this.model = model;
    this.tokenUsage = {
      totalTokens: 0,
      promptTokens: 0,
      completionTokens: 0,
      requestCount: 0,
      successCount: 0,
      failureCount: 0,
      averageLatency: 0,
    };
    Logger.debug('LLMClient initialized', { host, model });
  }

  async checkHealth(): Promise<boolean> {
    try {
      const response = await this.client.get('/api/tags');
      const models = (response.data as { models?: Array<{ name: string }> }).models || [];
      const hasModel = models.some((m) => m.name.includes(this.model));
      if (!hasModel) {
        Logger.warn('Model not found in Ollama', {
          model: this.model,
          available: models.map((m) => m.name),
        });
        return false;
      }
      Logger.info('LLM health check passed', { model: this.model });
      return true;
    } catch (error) {
      Logger.error('LLM health check failed', {
        error: error instanceof Error ? error.message : 'Unknown error',
      });
      return false;
    }
  }

  async generate(prompt: string, retries = 2): Promise<string> {
    const startTime = Date.now();
    const baseDelay = 1000;
    const maxDelay = 10000;

    for (let attempt = 1; attempt <= retries; attempt++) {
      try {
        const payload = {
          model: this.model,
          prompt,
          stream: false,
        };

        Logger.debug('Sending LLM request', {
          attempt,
          model: this.model,
          promptLength: prompt.length,
        });

        const response = await this.client.post('/api/generate', payload);
        const data = response.data as OllamaGenerateResponse;
        const latency = Date.now() - startTime;
        if (attempt > 1) {
          Logger.info('LLM request succeeded after retry', { attempt });
        }

        this.updateTokenUsage(prompt, data.response, data, latency, true);

        Logger.debug('LLM response received', {
          latency,
          promptEvalCount: data.prompt_eval_count,
          evalCount: data.eval_count,
        });

        return data.response;
      } catch (error) {
        const isLastAttempt = attempt === retries;
        const latency = Date.now() - startTime;
        Logger.warn('LLM request failed', {
          attempt,
          maxRetries: retries,
          error: error instanceof Error ? error.message : 'Unknown',
        });
        if (isLastAttempt) {
          this.updateTokenUsage(prompt, '', null, latency, false);
          throw error;
        }
        const delay = Math.min(baseDelay * Math.pow(2, attempt - 1), maxDelay);
        Logger.debug('Retrying after delay', { delay, nextAttempt: attempt + 1 });
        await new Promise((resolve) => setTimeout(resolve, delay));
      }
    }

    throw new Error('All retry attempts exhausted');
  }

  private updateTokenUsage(
    prompt: string,
    response: string,
    ollamaData: OllamaGenerateResponse | null,
    latency: number,
    success: boolean
  ): void {
    const previousTotal = this.tokenUsage.totalTokens;
    this.tokenUsage.requestCount++;

    if (success) {
      this.tokenUsage.successCount++;
      if (ollamaData?.prompt_eval_count && ollamaData?.eval_count) {
        this.tokenUsage.promptTokens += ollamaData.prompt_eval_count;
        this.tokenUsage.completionTokens += ollamaData.eval_count;
        Logger.debug('Token usage from API', {
          promptTokens: ollamaData.prompt_eval_count,
          completionTokens: ollamaData.eval_count,
        });
      } else {
        const estimatedPrompt = Math.ceil(prompt.length / 3.5);
        const estimatedCompletion = Math.ceil(response.length / 3.5);
        this.tokenUsage.promptTokens += estimatedPrompt;
        this.tokenUsage.completionTokens += estimatedCompletion;
        Logger.debug('Token usage estimated', {
          estimatedPrompt,
          estimatedCompletion,
        });
      }
      this.tokenUsage.totalTokens = this.tokenUsage.promptTokens + this.tokenUsage.completionTokens;
    } else {
      this.tokenUsage.failureCount++;
    }

    const n = this.tokenUsage.requestCount;
    this.tokenUsage.averageLatency = (this.tokenUsage.averageLatency * (n - 1) + latency) / n;

    Logger.debug('Token usage updated', {
      requestNumber: this.tokenUsage.requestCount,
      previousTotal,
      newTotal: this.tokenUsage.totalTokens,
      delta: this.tokenUsage.totalTokens - previousTotal,
      source: ollamaData?.prompt_eval_count ? 'api' : 'estimated',
    });
  }

  getTokenUsage(): TokenUsage {
    return { ...this.tokenUsage };
  }

  getModel(): string {
    return this.model;
  }

  resetTokenUsage(): void {
    this.tokenUsage = {
      totalTokens: 0,
      promptTokens: 0,
      completionTokens: 0,
      requestCount: 0,
      successCount: 0,
      failureCount: 0,
      averageLatency: 0,
    };
    Logger.info('Token usage reset');
  }
}
